---
title: "Pandas æ•ˆèƒ½èª¿æ ¡ï¼šè®“ä½ çš„è³‡æ–™åˆ†æé£›èµ·ä¾†ï¼"
date: 2025-09-27 19:32:37 +0800  # å¯¦éš›å»ºç«‹æ™‚é–“
last_modified_at: 2025-09-28 18:04:54 +0800  # æœ€å¾Œä¿®æ”¹æ™‚é–“ï¼ˆåŠ å…¥æ’åºåŠŸèƒ½ï¼‰
categories: [technical]
tags: [Python, Pandas, æ•ˆèƒ½å„ªåŒ–, è³‡æ–™åˆ†æ, DataFrame, è³‡æ–™ç§‘å­¸, æœ€ä½³å¯¦å‹™]
header:
  overlay_color: "#3498db"
  overlay_filter: "0.5"
  overlay_image: /assets/images/pandas-optimization-header.jpg
  teaser: /assets/images/pandas-optimization-teaser.jpg
excerpt: "æ˜¯ä¸æ˜¯å¸¸å¸¸ç­‰ Pandas è·‘è³‡æ–™è·‘åˆ°æ‡·ç–‘äººç”Ÿï¼ŸğŸŒ ä»Šå¤©ä¾†åˆ†äº«å¹¾å€‹å¯¦æˆ°æŠ€å·§ï¼Œè®“ä½ çš„ DataFrame æ“ä½œé€Ÿåº¦æå‡ 10 å€ï¼"
toc: true
toc_sticky: true
---

å‰å¹¾å¤©åŒäº‹è·‘ä¾†å•æˆ‘ï¼šã€Œç‚ºä»€éº¼æˆ‘çš„ Pandas ç¨‹å¼è·‘äº† 2 å°æ™‚é‚„æ²’çµæŸï¼Ÿã€çœ‹äº†ä¸€ä¸‹ä»–çš„ç¨‹å¼ç¢¼ï¼Œæˆ‘å·®é»ç¬‘å‡ºä¾†... ğŸ˜…

å…¶å¯¦ Pandas æ•ˆèƒ½å„ªåŒ–å°±åƒèª¿éŸ³éŸ¿ä¸€æ¨£ï¼ŒçŸ¥é“å¹¾å€‹é—œéµçš„ã€Œæ—‹éˆ•ã€åœ¨å“ªè£¡ï¼Œå°±èƒ½è®“å®ƒå”±å‡ºç¾å¦™çš„æ­Œè²ï¼ä»Šå¤©å°±ä¾†åˆ†äº«ä¸€äº›æˆ‘é€™å¹¾å¹´è¸©å‘ç´¯ç©çš„å„ªåŒ–æŠ€å·§ã€‚

<!--more-->

## ç‚ºä»€éº¼ Pandas æœƒé€™éº¼æ…¢ï¼Ÿ ğŸ¤”

é¦–å…ˆè¦äº†è§£ Pandas çš„ã€Œç—›é»ã€åœ¨å“ªè£¡ï¼š

### 1. è¨˜æ†¶é«”ä½¿ç”¨ä¸ç•¶
```python
import pandas as pd
import numpy as np

# ç”¢ç”Ÿæ¸¬è©¦è³‡æ–™
df = pd.DataFrame({
    'id': range(1000000),
    'value': np.random.randn(1000000)
})

# æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
print(df.info(memory_usage='deep'))
```

### 2. è¿´åœˆçš„é™·é˜±
```python
# âŒ é€™æ¨£å¯«æœƒè¢«åŒäº‹ç¿»ç™½çœ¼...
total = 0
for idx, row in df.iterrows():  # è¶…ç´šæ…¢ï¼
    total += row['value']

# âœ… å‘é‡åŒ–æ“ä½œ
total = df['value'].sum()  # å¿« 100 å€ï¼
```

è®“æˆ‘ç”¨å¯¦éš›æ¸¬è©¦ä¾†å±•ç¤ºå·®ç•°ï¼š

```python
import time

def time_it(func):
    start = time.time()
    result = func()
    end = time.time()
    print(f"åŸ·è¡Œæ™‚é–“: {end - start:.4f} ç§’")
    return result

# æ¸¬è©¦è³‡æ–™
df = pd.DataFrame({
    'A': np.random.randn(100000),
    'B': np.random.randn(100000)
})

# æ–¹æ³• 1: è¿´åœˆ (æ…¢åˆ°çˆ†)
def loop_method():
    result = []
    for idx, row in df.iterrows():
        result.append(row['A'] + row['B'])
    return result

# æ–¹æ³• 2: å‘é‡åŒ–æ“ä½œ (ç¥é€Ÿ)
def vectorized_method():
    return df['A'] + df['B']

print("è¿´åœˆæ–¹æ³•:")
time_it(loop_method)        # ~15 ç§’

print("å‘é‡åŒ–æ–¹æ³•:")
time_it(vectorized_method)  # ~0.001 ç§’
```

å·®ç•°æœ‰å¤šå¤§ï¼Ÿå‘é‡åŒ–æ“ä½œå¯ä»¥å¿«ä¸Š **15,000 å€**ï¼ğŸ˜±

## è³‡æ–™é¡å‹å„ªåŒ–ï¼šçœè¨˜æ†¶é«”å°±æ˜¯çœæ™‚é–“ ğŸ’¾

### é¸å°è³‡æ–™é¡å‹

```python
# å»ºç«‹æ¸¬è©¦ DataFrame
df = pd.DataFrame({
    'int_col': np.random.randint(0, 100, 1000000),
    'float_col': np.random.randn(1000000),
    'str_col': ['category_' + str(i % 10) for i in range(1000000)]
})

print("åŸå§‹è³‡æ–™é¡å‹:")
print(df.dtypes)
print(f"è¨˜æ†¶é«”ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# å„ªåŒ–è³‡æ–™é¡å‹
df_optimized = df.copy()

# int64 -> int8 (å¦‚æœæ•¸å€¼ç¯„åœå…è¨±)
df_optimized['int_col'] = df_optimized['int_col'].astype('int8')

# float64 -> float32 (é€šå¸¸ç²¾åº¦å¤ ç”¨)
df_optimized['float_col'] = df_optimized['float_col'].astype('float32')

# å­—ä¸² -> category (é‡è¤‡å€¼å¾ˆå¤šæ™‚è¶…æœ‰æ•ˆ)
df_optimized['str_col'] = df_optimized['str_col'].astype('category')

print("\nå„ªåŒ–å¾Œè³‡æ–™é¡å‹:")
print(df_optimized.dtypes)
print(f"è¨˜æ†¶é«”ä½¿ç”¨: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

çµæœé€šå¸¸èƒ½çœä¸‹ **50-70%** çš„è¨˜æ†¶é«”ï¼

### è‡ªå‹•å„ªåŒ–å‡½æ•¸

æˆ‘å¯«äº†ä¸€å€‹è‡ªå‹•å„ªåŒ–çš„å‡½æ•¸ï¼Œè¶…å¥½ç”¨çš„ï¼š

```python
def optimize_dtypes(df):
    """è‡ªå‹•å„ªåŒ– DataFrame çš„è³‡æ–™é¡å‹"""
    optimized = df.copy()
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != 'object':
            c_min = df[col].min()
            c_max = df[col].max()
            
            if str(col_type)[:3] == 'int':
                # æ•´æ•¸å‹å„ªåŒ–
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    optimized[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    optimized[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    optimized[col] = df[col].astype(np.int32)
                    
            elif str(col_type)[:5] == 'float':
                # æµ®é»æ•¸å„ªåŒ–
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    optimized[col] = df[col].astype(np.float32)
        else:
            # å­—ä¸²é¡å‹å„ªåŒ–
            num_unique_values = len(df[col].unique())
            num_total_values = len(df[col])
            if num_unique_values / num_total_values < 0.5:  # é‡è¤‡ç‡è¶…é 50%
                optimized[col] = df[col].astype('category')
    
    return optimized

# ä½¿ç”¨ç¯„ä¾‹
df_auto_optimized = optimize_dtypes(df)
```

## è®€å–è³‡æ–™çš„å„ªåŒ–æŠ€å·§ ğŸ“‚

### CSV è®€å–å„ªåŒ–

```python
# âŒ æ…¢é€Ÿè®€å–
df_slow = pd.read_csv('large_file.csv')

# âœ… å„ªåŒ–è®€å–
df_fast = pd.read_csv(
    'large_file.csv',
    dtype={'user_id': 'int32', 'category': 'category'},  # é å…ˆæŒ‡å®šå‹åˆ¥
    parse_dates=['timestamp'],  # æŒ‡å®šæ—¥æœŸæ¬„ä½
    nrows=10000,  # å…ˆè®€ä¸€éƒ¨åˆ†æ¸¬è©¦
    chunksize=10000,  # åˆ†æ‰¹è®€å–
    low_memory=False,  # é¿å…æ··åˆå‹åˆ¥æ¨æ–·
    engine='c'  # ä½¿ç”¨ C å¼•æ“ (é è¨­)
)
```

### åˆ†æ‰¹è™•ç†å¤§æª”æ¡ˆ

```python
def process_large_csv(filename, chunk_size=10000):
    """åˆ†æ‰¹è™•ç†å¤§å‹ CSV æª”æ¡ˆ"""
    results = []
    
    for chunk in pd.read_csv(filename, chunksize=chunk_size):
        # å°æ¯å€‹ chunk é€²è¡Œè™•ç†
        processed_chunk = chunk.groupby('category').sum()
        results.append(processed_chunk)
    
    # åˆä½µæ‰€æœ‰çµæœ
    final_result = pd.concat(results).groupby(level=0).sum()
    return final_result

# ä½¿ç”¨ç¯„ä¾‹
result = process_large_csv('huge_sales_data.csv')
```

## å‘é‡åŒ–æ“ä½œï¼šæ‹’çµ•è¿´åœˆçš„èª˜æƒ‘ ğŸš€

### apply() vs å‘é‡åŒ–æ“ä½œ

```python
# æ¸¬è©¦è³‡æ–™
df = pd.DataFrame({
    'A': np.random.randn(100000),
    'B': np.random.randn(100000),
    'C': np.random.choice(['X', 'Y', 'Z'], 100000)
})

# âŒ ä½¿ç”¨ apply (è¼ƒæ…¢)
def slow_calculation(row):
    if row['C'] == 'X':
        return row['A'] + row['B']
    elif row['C'] == 'Y': 
        return row['A'] - row['B']
    else:
        return row['A'] * row['B']

df['result_slow'] = df.apply(slow_calculation, axis=1)

# âœ… å‘é‡åŒ–æ“ä½œ (è¶…å¿«)
conditions = [
    df['C'] == 'X',
    df['C'] == 'Y',
    df['C'] == 'Z'
]

choices = [
    df['A'] + df['B'],
    df['A'] - df['B'], 
    df['A'] * df['B']
]

df['result_fast'] = np.select(conditions, choices)
```

### å­—ä¸²æ“ä½œå„ªåŒ–

```python
# âŒ æ…¢é€Ÿå­—ä¸²è™•ç†
df['processed'] = df['text_col'].apply(lambda x: x.upper().strip())

# âœ… å‘é‡åŒ–å­—ä¸²æ“ä½œ
df['processed'] = df['text_col'].str.upper().str.strip()

# æ›´è¤‡é›œçš„ä¾‹å­ï¼šæå– email ç¶²åŸŸ
# âŒ æ…¢é€Ÿç‰ˆæœ¬
df['domain'] = df['email'].apply(lambda x: x.split('@')[1] if '@' in x else '')

# âœ… å¿«é€Ÿç‰ˆæœ¬
df['domain'] = df['email'].str.split('@').str[1].fillna('')
```

## ç¾¤çµ„æ“ä½œå„ªåŒ– ğŸ“Š

### GroupBy æ•ˆèƒ½èª¿æ ¡

```python
# æ¸¬è©¦è³‡æ–™
df = pd.DataFrame({
    'group': np.random.choice(['A', 'B', 'C'], 1000000),
    'value1': np.random.randn(1000000),
    'value2': np.random.randn(1000000),
    'date': pd.date_range('2020-01-01', periods=1000000, freq='1min')
})

# âŒ æ…¢é€Ÿç¾¤çµ„æ“ä½œ
def slow_groupby():
    result = []
    for group in df['group'].unique():
        subset = df[df['group'] == group]
        agg_result = {
            'group': group,
            'mean_value1': subset['value1'].mean(),
            'sum_value2': subset['value2'].sum()
        }
        result.append(agg_result)
    return pd.DataFrame(result)

# âœ… å¿«é€Ÿç¾¤çµ„æ“ä½œ
def fast_groupby():
    return df.groupby('group').agg({
        'value1': 'mean',
        'value2': 'sum'
    }).reset_index()

# æ•ˆèƒ½æ¸¬è©¦
print("æ…¢é€Ÿæ–¹æ³•:")
time_it(slow_groupby)    # ~2 ç§’

print("å¿«é€Ÿæ–¹æ³•:")
time_it(fast_groupby)    # ~0.05 ç§’
```

### é€²éš GroupBy æŠ€å·§

```python
# å¤šé‡èšåˆæ“ä½œ
agg_functions = {
    'value1': ['mean', 'std', 'min', 'max'],
    'value2': ['sum', 'count'],
    'date': ['min', 'max']
}

result = df.groupby('group').agg(agg_functions)

# æ‰å¹³åŒ–æ¬„ä½åç¨±
result.columns = ['_'.join(col).strip() for col in result.columns]
result = result.reset_index()

print(result.head())
```

## è¨˜æ†¶é«”ç®¡ç†èˆ‡ç›£æ§ ğŸ“ˆ

### ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨

```python
def memory_usage_check(df, message=""):
    """æª¢æŸ¥ DataFrame è¨˜æ†¶é«”ä½¿ç”¨"""
    memory_mb = df.memory_usage(deep=True).sum() / 1024**2
    print(f"{message} è¨˜æ†¶é«”ä½¿ç”¨: {memory_mb:.2f} MB")
    return memory_mb

# ä½¿ç”¨ç¯„ä¾‹
original_memory = memory_usage_check(df, "åŸå§‹è³‡æ–™")
optimized_memory = memory_usage_check(df_optimized, "å„ªåŒ–å¾Œ")

print(f"ç¯€çœè¨˜æ†¶é«”: {((original_memory - optimized_memory) / original_memory) * 100:.1f}%")
```

### å³æ™‚è¨˜æ†¶é«”æ¸…ç†

```python
import gc

def cleanup_memory():
    """å¼·åˆ¶åƒåœ¾å›æ”¶"""
    gc.collect()
    
# åœ¨è™•ç†å¤§è³‡æ–™æ™‚å®šæœŸæ¸…ç†
for chunk in pd.read_csv('large_file.csv', chunksize=10000):
    # è™•ç†è³‡æ–™
    processed = process_chunk(chunk)
    
    # æ¸…ç†è¨˜æ†¶é«”
    del chunk
    cleanup_memory()
```

## å¹³è¡Œè™•ç†ï¼šå¤šæ ¸å¿ƒå¨åŠ› ğŸ’ª

### ä½¿ç”¨ Dask è™•ç†è¶…å¤§è³‡æ–™

```python
import dask.dataframe as dd

# è®€å–å¤§å‹è³‡æ–™é›†
dask_df = dd.read_csv('massive_file.csv')

# Dask æœƒè‡ªå‹•åˆ†å‰²è³‡æ–™åˆ°å¤šå€‹æ ¸å¿ƒ
result = dask_df.groupby('category').value.mean().compute()

# æ¯”è¼ƒ Pandas vs Dask
def pandas_process():
    df = pd.read_csv('large_file.csv')  # å¯èƒ½æœƒ out of memory
    return df.groupby('category').value.mean()

def dask_process():
    df = dd.read_csv('large_file.csv')  # åˆ†æ‰¹è®€å–
    return df.groupby('category').value.mean().compute()
```

### å¤šé€²ç¨‹è™•ç†

```python
from multiprocessing import Pool
import numpy as np

def process_chunk(chunk):
    """è™•ç†å–®å€‹ chunk çš„å‡½æ•¸"""
    return chunk.groupby('group').sum()

def parallel_processing(df, num_processes=4):
    """å¹³è¡Œè™•ç† DataFrame"""
    
    # åˆ†å‰²è³‡æ–™
    chunks = np.array_split(df, num_processes)
    
    # ä½¿ç”¨å¤šé€²ç¨‹è™•ç†
    with Pool(processes=num_processes) as pool:
        results = pool.map(process_chunk, chunks)
    
    # åˆä½µçµæœ
    final_result = pd.concat(results).groupby(level=0).sum()
    return final_result

# ä½¿ç”¨ç¯„ä¾‹
result = parallel_processing(large_df, num_processes=8)
```

## æˆ‘çš„è¸©å‘è¡€æ·šå² ğŸ’€

### 1. SettingWithCopyWarning çš„é™·é˜±

```python
# âŒ é€™æ¨£æœƒæœ‰è­¦å‘Šï¼Œè€Œä¸”å¯èƒ½ä¸æœƒç”Ÿæ•ˆ
df_subset = df[df['value'] > 0]
df_subset['new_col'] = 'something'  # è­¦å‘Šï¼

# âœ… æ­£ç¢ºçš„åšæ³•
df_subset = df[df['value'] > 0].copy()  # æ˜ç¢ºè¤‡è£½
df_subset['new_col'] = 'something'  # OK!

# æˆ–è€…ç›´æ¥åœ¨åŸ DataFrame æ“ä½œ
df.loc[df['value'] > 0, 'new_col'] = 'something'
```

### 2. æ—¥æœŸæ™‚é–“è™•ç†çš„å‘

```python
# âŒ å­—ä¸²æ¯”è¼ƒ (è¶…æ…¢)
df['date_str'] = df['date'].astype(str)
result = df[df['date_str'] > '2023-01-01']

# âœ… ç›´æ¥ç”¨ datetime æ¯”è¼ƒ (è¶…å¿«)
df['date'] = pd.to_datetime(df['date'])
result = df[df['date'] > '2023-01-01']
```

### 3. join vs merge çš„é¸æ“‡

```python
# ç•¶æœ‰æ˜ç¢ºçš„ç´¢å¼•é—œä¿‚æ™‚ï¼Œjoin æ¯” merge å¿«å¾ˆå¤š
df1 = df1.set_index('key_col')
df2 = df2.set_index('key_col')

# âœ… ä½¿ç”¨ join (è¼ƒå¿«)
result = df1.join(df2, how='inner')

# è€Œä¸æ˜¯ merge (è¼ƒæ…¢)
# result = pd.merge(df1, df2, on='key_col', how='inner')
```

## å¯¦æˆ°æ¡ˆä¾‹ï¼šè‚¡ç¥¨è³‡æ–™åˆ†æå„ªåŒ– ğŸ“ˆ

è®“æˆ‘ç”¨ä¸€å€‹å¯¦éš›ä¾‹å­ä¾†å±•ç¤ºé€™äº›æŠ€å·§ï¼š

```python
def analyze_stock_data_slow(df):
    """æœªå„ªåŒ–ç‰ˆæœ¬"""
    results = []
    
    for stock in df['symbol'].unique():
        stock_data = df[df['symbol'] == stock].copy()
        
        # è¨ˆç®—ç§»å‹•å¹³å‡ (æ…¢)
        ma_5 = []
        ma_20 = []
        for i in range(len(stock_data)):
            if i >= 4:
                ma_5.append(stock_data.iloc[i-4:i+1]['close'].mean())
            else:
                ma_5.append(np.nan)
                
            if i >= 19:
                ma_20.append(stock_data.iloc[i-19:i+1]['close'].mean())
            else:
                ma_20.append(np.nan)
        
        stock_data['ma_5'] = ma_5
        stock_data['ma_20'] = ma_20
        results.append(stock_data)
    
    return pd.concat(results)

def analyze_stock_data_fast(df):
    """å„ªåŒ–ç‰ˆæœ¬"""
    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œè¨ˆç®—ç§»å‹•å¹³å‡
    df = df.sort_values(['symbol', 'date'])
    df['ma_5'] = df.groupby('symbol')['close'].rolling(5).mean().values
    df['ma_20'] = df.groupby('symbol')['close'].rolling(20).mean().values
    
    return df

# æ•ˆèƒ½æ¯”è¼ƒ
stock_df = pd.DataFrame({
    'symbol': np.random.choice(['AAPL', 'GOOGL', 'MSFT'], 10000),
    'date': pd.date_range('2020-01-01', periods=10000),
    'close': np.random.randn(10000) * 100 + 1000
})

print("æœªå„ªåŒ–ç‰ˆæœ¬:")
time_it(lambda: analyze_stock_data_slow(stock_df))  # ~15 ç§’

print("å„ªåŒ–ç‰ˆæœ¬:")
time_it(lambda: analyze_stock_data_fast(stock_df))   # ~0.1 ç§’
```

## æ•ˆèƒ½ç›£æ§èˆ‡ Profiling ğŸ”

### ä½¿ç”¨ cProfile æ‰¾ç“¶é ¸

```python
import cProfile
import pstats

def profile_pandas_code():
    """è¦åˆ†æçš„ Pandas ç¨‹å¼ç¢¼"""
    df = pd.DataFrame(np.random.randn(100000, 10))
    result = df.groupby(df.index % 1000).sum()
    return result

# æ•ˆèƒ½åˆ†æ
cProfile.run('profile_pandas_code()', 'pandas_profile.prof')

# æŸ¥çœ‹çµæœ
stats = pstats.Stats('pandas_profile.prof')
stats.sort_stats('cumulative').print_stats(10)
```

### ä½¿ç”¨ line_profiler é€è¡Œåˆ†æ

```bash
# å®‰è£ line_profiler
pip install line_profiler
```

```python
@profile  # éœ€è¦ line_profiler
def detailed_analysis(df):
    # æ¯ä¸€è¡Œéƒ½æœƒè¢«åˆ†æ
    grouped = df.groupby('category')  # é€™è¡ŒèŠ±å¤šå°‘æ™‚é–“ï¼Ÿ
    result = grouped.sum()            # é€™è¡ŒèŠ±å¤šå°‘æ™‚é–“ï¼Ÿ 
    return result
```

## ç¸½çµèˆ‡æœ€ä½³å¯¦å‹™ ğŸ¯

æ ¹æ“šæˆ‘çš„ç¶“é©—ï¼ŒPandas æ•ˆèƒ½å„ªåŒ–çš„é»ƒé‡‘æ³•å‰‡ï¼š

### 1. å„ªå…ˆé †åº
1. **è³‡æ–™é¡å‹å„ªåŒ–** - æŠ•è³‡å ±é…¬ç‡æœ€é«˜
2. **å‘é‡åŒ–æ“ä½œ** - é¿å…è¿´åœˆ
3. **åˆ†æ‰¹è™•ç†** - æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨
4. **å¹³è¡Œè™•ç†** - å–„ç”¨å¤šæ ¸å¿ƒ

### 2. é–‹ç™¼ç¿’æ…£
- âœ… ç¸½æ˜¯å…ˆç”¨å°è³‡æ–™é›†æ¸¬è©¦
- âœ… å®šæœŸæª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
- âœ… ä½¿ç”¨ `.copy()` é¿å… SettingWithCopyWarning
- âœ… å„ªå…ˆè€ƒæ…®å‘é‡åŒ–æ“ä½œ

### 3. æ•ˆèƒ½æª¢æŸ¥æ¸…å–®

```python
def performance_checklist(df):
    """Pandas æ•ˆèƒ½æª¢æŸ¥æ¸…å–®"""
    
    print("ğŸ” Pandas æ•ˆèƒ½æª¢æŸ¥å ±å‘Š")
    print("=" * 40)
    
    # 1. è³‡æ–™é¡å‹æª¢æŸ¥
    print("\n1. è³‡æ–™é¡å‹åˆ†æ:")
    memory_usage = df.memory_usage(deep=True).sum() / 1024**2
    print(f"   ç¸½è¨˜æ†¶é«”ä½¿ç”¨: {memory_usage:.2f} MB")
    
    object_cols = df.select_dtypes(include=['object']).columns
    if len(object_cols) > 0:
        print(f"   âš ï¸  ç™¼ç¾ {len(object_cols)} å€‹ object æ¬„ä½ï¼Œè€ƒæ…®è½‰æ›ç‚º category")
    
    # 2. é‡è¤‡å€¼æª¢æŸ¥
    print("\n2. é‡è¤‡å€¼åˆ†æ:")
    for col in df.columns:
        unique_ratio = df[col].nunique() / len(df)
        if unique_ratio < 0.5 and df[col].dtype == 'object':
            print(f"   ğŸ’¡ {col}: é‡è¤‡ç‡ {(1-unique_ratio)*100:.1f}%ï¼Œå»ºè­°è½‰ç‚º category")
    
    # 3. ç©ºå€¼æª¢æŸ¥
    print("\n3. ç©ºå€¼åˆ†æ:")
    null_cols = df.isnull().sum()
    for col, null_count in null_cols[null_cols > 0].items():
        print(f"   ğŸ“Š {col}: {null_count} å€‹ç©ºå€¼ ({null_count/len(df)*100:.1f}%)")

# ä½¿ç”¨ç¯„ä¾‹
performance_checklist(df)
```

è¨˜ä½ï¼Œ**æ•ˆèƒ½å„ªåŒ–æ˜¯ä¸€é–€å¹³è¡¡çš„è—è¡“**ã€‚ä¸è¦ç‚ºäº†å¾®å°çš„æ•ˆèƒ½æå‡è€ŒçŠ§ç‰²ç¨‹å¼ç¢¼çš„å¯è®€æ€§ã€‚å…ˆè®“ç¨‹å¼è·‘èµ·ä¾†ï¼Œå†è®“ç¨‹å¼è·‘å¾—å¿«ï¼

---

## å»¶ä¼¸é–±è®€ ğŸ“š

- ğŸ“– [Pandas Documentation - Performance](https://pandas.pydata.org/docs/user_guide/enhancingperf.html)
- ğŸ¥ [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
- ğŸ’» [Dask Documentation](https://docs.dask.org/en/stable/)

ä¸‹æ¬¡ä¾†èŠèŠ NumPy çš„æ•ˆèƒ½å„ªåŒ–æŠ€å·§ï¼Œé‚£å€‹æ›´æ˜¯æ·±ä¸è¦‹åº•çš„å‘ ğŸ˜‚

æœ‰ä»»ä½•å•é¡Œæ­¡è¿ç•™è¨€è¨è«–ï¼è®“æˆ‘å€‘ä¸€èµ·è®“ Pandas é£›èµ·ä¾† ğŸš€